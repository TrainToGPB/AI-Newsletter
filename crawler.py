#!/usr/bin/env python3
"""
Newsletter Source Crawler
í¬ë¡¤ë§í•œ í˜ì´ì§€ì˜ êµ¬ì¡°ë¥¼ JSON í˜•íƒœë¡œ íŒŒì‹±í•˜ì—¬ ì¶œë ¥í•©ë‹ˆë‹¤.
"""

import json
import re
import time
import random
import requests
import hashlib
import asyncio
import aiohttp
from datetime import datetime
from abc import ABC, abstractmethod
from dataclasses import dataclass, asdict
from typing import Dict, List, Any, Optional
from pathlib import Path

import trafilatura
from urllib.parse import urljoin, urlparse


@dataclass
class Article:
    """ê¸°ì‚¬/ë…¼ë¬¸ ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    title: str
    url: str
    date: Optional[str] = None
    author: Optional[str] = None
    description: Optional[str] = None
    content: Optional[str] = None
    tags: Optional[List[str]] = None
    metadata: Optional[Dict[str, Any]] = None

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {k: v for k, v in asdict(self).items() if v is not None}


class BaseSourceCrawler(ABC):
    """ì†ŒìŠ¤ í¬ë¡¤ëŸ¬ì˜ ê¸°ë³¸ í´ë˜ìŠ¤"""

    def __init__(self, source_name: str, base_url: str, rate_limit: float = 1.0):
        self.source_name = source_name
        self.base_url = base_url
        self.rate_limit = rate_limit  # ìš”ì²­ ê°„ ìµœì†Œ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        self.last_request_time = 0
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def _rate_limit_wait(self):
        """ì†ë„ ì œí•œì„ ìœ„í•œ ëŒ€ê¸°"""
        elapsed = time.time() - self.last_request_time
        if elapsed < self.rate_limit:
            wait_time = self.rate_limit - elapsed + random.uniform(0.1, 0.5)
            time.sleep(wait_time)
        self.last_request_time = time.time()

    def fetch_page(self, url: str) -> Optional[str]:
        """í˜ì´ì§€ HTML ê°€ì ¸ì˜¤ê¸°"""
        try:
            self._rate_limit_wait()
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"Error fetching {url}: {e}")
            return None

    def extract_with_trafilatura(self, html: str, url: str) -> Dict[str, Any]:
        """trafilaturaë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ"""
        downloaded = trafilatura.extract(
            html,
            include_comments=False,
            include_tables=True,
            include_links=True,
            output_format='json',
            url=url
        )

        if downloaded:
            return json.loads(downloaded)
        return {}

    def extract_metadata(self, html: str) -> Dict[str, Any]:
        """trafilaturaë¡œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        return trafilatura.extract_metadata(html) or {}

    def extract_date_from_article(self, url: str) -> Optional[str]:
        """ê°œë³„ ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ë‚ ì§œ ì¶”ì¶œ"""
        try:
            html = self.fetch_page(url)
            if not html:
                return None

            # trafilaturaë¡œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = trafilatura.extract_metadata(html)
            if metadata and metadata.date:
                return metadata.date

            # JSON-LD êµ¬ì¡°í™”ëœ ë°ì´í„°ì—ì„œ ë‚ ì§œ ì°¾ê¸°
            from lxml import html as lxml_html
            import re

            tree = lxml_html.fromstring(html)

            # JSON-LD script íƒœê·¸ ì°¾ê¸°
            json_ld_scripts = tree.xpath('//script[@type="application/ld+json"]')
            for script in json_ld_scripts:
                try:
                    data = json.loads(script.text_content())
                    if isinstance(data, dict):
                        # datePublished, publishedDate, dateCreated ë“± ì°¾ê¸°
                        for key in ['datePublished', 'publishedDate', 'dateCreated', 'date']:
                            if key in data:
                                return data[key]
                    elif isinstance(data, list):
                        for item in data:
                            if isinstance(item, dict):
                                for key in ['datePublished', 'publishedDate', 'dateCreated', 'date']:
                                    if key in item:
                                        return item[key]
                except:
                    continue

            # meta íƒœê·¸ì—ì„œ ë‚ ì§œ ì°¾ê¸°
            meta_date_selectors = [
                '//meta[@property="article:published_time"]/@content',
                '//meta[@name="publication_date"]/@content',
                '//meta[@name="date"]/@content',
                '//meta[@property="og:published_time"]/@content',
                '//meta[@name="pubdate"]/@content',
            ]

            for selector in meta_date_selectors:
                dates = tree.xpath(selector)
                if dates:
                    return dates[0]

            # time íƒœê·¸ì—ì„œ ë‚ ì§œ ì°¾ê¸°
            time_tags = tree.xpath('//time/@datetime')
            if time_tags:
                return time_tags[0]

            return None
        except Exception as e:
            print(f"Error extracting date from {url}: {e}")
            return None

    @abstractmethod
    def parse_listing_page(self, html: str) -> List[Article]:
        """ëª©ë¡ í˜ì´ì§€ë¥¼ íŒŒì‹±í•˜ì—¬ Article ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        pass

    def crawl(self, full_content: bool = False, extract_dates: bool = True) -> Dict[str, Any]:
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        print(f"\n{'='*60}")
        print(f"Crawling: {self.source_name}")
        print(f"URL: {self.base_url}")
        print(f"{'='*60}\n")

        html = self.fetch_page(self.base_url)
        if not html:
            return {
                "source": self.source_name,
                "url": self.base_url,
                "error": "Failed to fetch page",
                "timestamp": datetime.now().isoformat()
            }

        # ëª©ë¡ í˜ì´ì§€ íŒŒì‹±
        articles = self.parse_listing_page(html)

        # ë‚ ì§œê°€ ì—†ëŠ” ê¸°ì‚¬ë“¤ì— ëŒ€í•´ ë‚ ì§œ ì¶”ì¶œ
        if extract_dates:
            articles_without_dates = [a for a in articles if not a.date]
            if articles_without_dates:
                print(f"Extracting dates for {len(articles_without_dates)} articles...")
                for i, article in enumerate(articles_without_dates, 1):
                    print(f"  [{i}/{len(articles_without_dates)}] {article.title[:50]}...")
                    date = self.extract_date_from_article(article.url)
                    if date:
                        article.date = date
                        print(f"    â†’ Found date: {date}")

        # ì „ì²´ ì»¨í…ì¸ ë¥¼ ê°€ì ¸ì˜¬ ê²½ìš°
        if full_content:
            print(f"Extracting full content for first 5 articles...")
            for article in articles[:5]:
                if article.url:
                    article_html = self.fetch_page(article.url)
                    if article_html:
                        extracted = self.extract_with_trafilatura(article_html, article.url)
                        article.content = extracted.get('text', '')

        result = {
            "source": self.source_name,
            "url": self.base_url,
            "timestamp": datetime.now().isoformat(),
            "articles_count": len(articles),
            "articles": [article.to_dict() for article in articles]
        }

        return result


class AlphaXivCrawler(BaseSourceCrawler):
    """alphaXiv í¬ë¡¤ëŸ¬ - Playwright ì‚¬ìš©"""

    def __init__(self):
        super().__init__("alphaxiv", "https://www.alphaxiv.org", rate_limit=2.0)

    def extract_date_from_article(self, url: str) -> Optional[str]:
        """AlphaXiv ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ë‚ ì§œ ì¶”ì¶œ - URL ë³€í™˜"""
        # /abs/ ë¥¼ /ko/overview/ ë¡œ ë³€ê²½
        if '/abs/' in url:
            url = url.replace('/abs/', '/ko/overview/')

        # ë¶€ëª¨ í´ë˜ìŠ¤ì˜ extract_date_from_article í˜¸ì¶œ
        return super().extract_date_from_article(url)

    def fetch_page(self, url: str) -> Optional[str]:
        """Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ JavaScript ë Œë”ë§ëœ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
        try:
            from playwright.sync_api import sync_playwright

            # ì†ë„ ì œí•œ ì ìš©
            self._rate_limit_wait()

            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                page = browser.new_page()

                print(f"Playwrightë¡œ í˜ì´ì§€ ë¡œë”© ì¤‘: {url}")
                page.goto(url, wait_until="networkidle", timeout=30000)

                # ë…¼ë¬¸ ëª©ë¡ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                page.wait_for_timeout(2000)

                html = page.content()
                browser.close()
                return html
        except Exception as e:
            print(f"Playwright í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None

    def parse_listing_page(self, html: str) -> List[Article]:
        """alphaXiv ëª©ë¡ í˜ì´ì§€ íŒŒì‹±"""
        from lxml import html as lxml_html

        articles = []
        seen_urls = set()
        tree = lxml_html.fromstring(html)

        # ë…¼ë¬¸ ì¹´ë“œë‚˜ ë§í¬ ì°¾ê¸°
        # alphaXivì˜ êµ¬ì¡°ì— ë§ê²Œ ì„ íƒì ì¡°ì •
        selectors = [
            "//a[contains(@href, '/abs/')]",  # arXiv ìŠ¤íƒ€ì¼ ë…¼ë¬¸ ë§í¬
            "//article//a",
            "//div[contains(@class, 'paper')]//a",
            "//div[contains(@class, 'card')]//a",
        ]

        for selector in selectors:
            links = tree.xpath(selector)
            if links:
                print(f"Found {len(links)} links with selector: {selector}")

                for link in links:
                    article = self._parse_paper_link(link)
                    if article and article.url not in seen_urls:
                        articles.append(article)
                        seen_urls.add(article.url)

                if articles:
                    break

        return articles[:20]  # ìµœëŒ€ 20ê°œë§Œ

    def _parse_paper_link(self, link) -> Optional[Article]:
        """ê°œë³„ ë…¼ë¬¸ ë§í¬ íŒŒì‹±"""
        try:
            title = link.text_content().strip()
            url = link.get('href', '')

            # ì œëª©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë„¤ë¹„ê²Œì´ì…˜ ë§í¬ëŠ” ìŠ¤í‚µ
            if not title or len(title) < 10:
                return None

            # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
            if url and not url.startswith('http'):
                url = urljoin(self.base_url, url)

            # ë¶€ëª¨ ìš”ì†Œì—ì„œ ë©”íƒ€ë°ì´í„° ì°¾ê¸°
            parent = link.getparent()
            date = None
            author = None
            description = None

            for _ in range(3):
                if parent is None:
                    break

                # ë‚ ì§œ ì°¾ê¸°
                if date is None:
                    date_elem = parent.xpath(".//*[contains(@class, 'date') or contains(@class, 'time')]")
                    if date_elem:
                        date = date_elem[0].text_content().strip()

                # ì €ì ì°¾ê¸°
                if author is None:
                    author_elem = parent.xpath(".//*[contains(@class, 'author')]")
                    if author_elem:
                        author = author_elem[0].text_content().strip()

                # ì„¤ëª…/ì´ˆë¡ ì°¾ê¸°
                if description is None:
                    desc_elem = parent.xpath(".//p")
                    if desc_elem:
                        desc_text = desc_elem[0].text_content().strip()
                        if desc_text and desc_text != title:
                            description = desc_text

                parent = parent.getparent()

            return Article(
                title=title,
                url=url,
                date=date,
                author=author,
                description=description
            )
        except Exception as e:
            print(f"Error parsing paper link: {e}")
            return None


class HuggingFaceBlogCrawler(BaseSourceCrawler):
    """Hugging Face Blog í¬ë¡¤ëŸ¬"""

    def __init__(self):
        super().__init__("hf_blog", "https://huggingface.co/blog", rate_limit=1.5)

    def parse_listing_page(self, html: str) -> List[Article]:
        """Hugging Face ë¸”ë¡œê·¸ ëª©ë¡ í˜ì´ì§€ íŒŒì‹±"""
        from lxml import html as lxml_html

        articles = []
        tree = lxml_html.fromstring(html)

        # Hugging Face ë¸”ë¡œê·¸ êµ¬ì¡° ë¶„ì„
        selectors = [
            "//article",
            "//div[contains(@class, 'blog')]//a[contains(@href, '/blog/')]",
            "//a[contains(@href, '/blog/') and .//h2]",
            "//div[contains(@class, 'post')]",
        ]

        for selector in selectors:
            elements = tree.xpath(selector)
            if elements:
                print(f"Found {len(elements)} elements with selector: {selector}")

                for elem in elements[:20]:
                    article = self._parse_blog_post(elem)
                    if article:
                        articles.append(article)

                if articles:
                    break

        return articles

    def _parse_blog_post(self, elem) -> Optional[Article]:
        """ê°œë³„ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ íŒŒì‹±"""
        try:
            title_elem = elem.xpath(".//h1 | .//h2 | .//h3")
            title = title_elem[0].text_content().strip() if title_elem else ""

            link_elem = elem if elem.tag == 'a' else elem.xpath(".//a[@href]")
            if isinstance(link_elem, list):
                url = link_elem[0].get('href', '') if link_elem else ""
            else:
                url = link_elem.get('href', '')

            if url and not url.startswith('http'):
                url = urljoin(self.base_url, url)

            if not title or not url:
                return None

            date_elem = elem.xpath(".//*[contains(@class, 'date') or contains(text(), '20')]")
            date = date_elem[0].text_content().strip() if date_elem else None

            author_elem = elem.xpath(".//*[contains(@class, 'author')]")
            author = author_elem[0].text_content().strip() if author_elem else None

            desc_elem = elem.xpath(".//p[not(ancestor::*[contains(@class, 'metadata')])]")
            description = desc_elem[0].text_content().strip() if desc_elem else None

            # íƒœê·¸ ì°¾ê¸°
            tag_elems = elem.xpath(".//*[contains(@class, 'tag')]")
            tags = [tag.text_content().strip() for tag in tag_elems] if tag_elems else None

            return Article(
                title=title,
                url=url,
                date=date,
                author=author,
                description=description,
                tags=tags
            )
        except Exception as e:
            print(f"Error parsing blog post: {e}")
            return None


class VentureBeatCrawler(BaseSourceCrawler):
    """VentureBeat AI í¬ë¡¤ëŸ¬"""

    def __init__(self):
        super().__init__("venturebeat", "https://venturebeat.com/category/ai/", rate_limit=2.0)

    def parse_listing_page(self, html: str) -> List[Article]:
        """VentureBeat ëª©ë¡ í˜ì´ì§€ íŒŒì‹±"""
        from lxml import html as lxml_html

        articles = []
        tree = lxml_html.fromstring(html)

        # article íƒœê·¸ì—ì„œ ê¸°ì‚¬ ì¶”ì¶œ
        article_elements = tree.xpath("//article")
        print(f"Found {len(article_elements)} article elements")

        for elem in article_elements:
            article = self._parse_article(elem)
            if article:
                articles.append(article)

        return articles

    def _parse_article(self, elem) -> Optional[Article]:
        """ê°œë³„ ê¸°ì‚¬ íŒŒì‹±"""
        try:
            # h2 ë˜ëŠ” h3 ì•ˆì˜ ë§í¬ì—ì„œ ì œëª© ì¶”ì¶œ
            title_elem = elem.xpath('.//h2//a | .//h3//a')
            if not title_elem:
                return None

            title = title_elem[0].text_content().strip()
            url = title_elem[0].get('href', '')

            if not title or not url:
                return None

            if url and not url.startswith('http'):
                url = urljoin(self.base_url, url)

            # ë‚ ì§œ ì°¾ê¸°
            date_elem = elem.xpath(".//*[contains(@class, 'date') or contains(@class, 'time')]")
            date = date_elem[0].text_content().strip() if date_elem else None

            # ì €ì ì°¾ê¸°
            author_elem = elem.xpath(".//*[contains(@class, 'author')]")
            author = author_elem[0].text_content().strip() if author_elem else None

            # ì„¤ëª… ì°¾ê¸° (p íƒœê·¸, ë‹¨ ì´ë¯¸ì§€ altëŠ” ì œì™¸)
            desc_elem = elem.xpath(".//p[not(parent::figcaption)]")
            description = desc_elem[0].text_content().strip() if desc_elem else None

            # ì¹´í…Œê³ ë¦¬/íƒœê·¸ ì°¾ê¸°
            tag_elems = elem.xpath(".//*[contains(@class, 'category') or contains(@class, 'tag')]")
            tags = [tag.text_content().strip() for tag in tag_elems] if tag_elems else None

            return Article(
                title=title,
                url=url,
                date=date,
                author=author,
                description=description,
                tags=tags
            )
        except Exception as e:
            print(f"Error parsing article: {e}")
            return None


def url_to_hash(url: str) -> str:
    """URLì„ ì§§ì€ hashë¡œ ë³€í™˜ (8ì)"""
    return hashlib.md5(url.encode()).hexdigest()[:8]


def fetch_article_content(articles: List[Dict[str, Any]], output_base: Path = Path("data/articles")) -> None:
    """
    ì„ íƒëœ ê¸°ì‚¬ë“¤ì˜ ë³¸ë¬¸ì„ markdownìœ¼ë¡œ ì¶”ì¶œí•˜ì—¬ ì €ì¥

    Args:
        articles: [{"source": "alphaxiv", "url": "...", "title": "...", "date": "..."}, ...]
        output_base: ì €ì¥ ê¸°ë³¸ ê²½ë¡œ
    """
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    })

    for article in articles:
        try:
            source = article.get('source', 'unknown')
            url = article.get('url', '')
            title = article.get('title', 'Untitled')
            date = article.get('date', '')

            if not url:
                print(f"âš ï¸  URLì´ ì—†ëŠ” ê¸°ì‚¬ ê±´ë„ˆëœ€: {title}")
                continue

            # ì†ŒìŠ¤ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
            source_dir = output_base / source
            source_dir.mkdir(parents=True, exist_ok=True)

            # URL hashë¡œ íŒŒì¼ëª… ìƒì„±
            filename = f"{url_to_hash(url)}.md"
            output_file = source_dir / filename

            # ì´ë¯¸ íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ê±´ë„ˆëœ€
            if output_file.exists():
                print(f"â­ï¸  ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ ê±´ë„ˆëœ€: {output_file}")
                continue

            print(f"\n{'='*60}")
            print(f"ğŸ“¥ Content ì¶”ì¶œ ì¤‘: {title[:50]}...")
            print(f"   Source: {source}")
            print(f"   URL: {url}")

            # alphaxivì˜ ê²½ìš° URL ë³€í™˜ (/abs/ â†’ /ko/overview/)
            fetch_url = url
            if source == 'alphaxiv' and '/abs/' in url:
                fetch_url = url.replace('/abs/', '/ko/overview/')
                print(f"   â†’ ë³€í™˜ëœ URL: {fetch_url}")

            # HTML ê°€ì ¸ì˜¤ê¸°
            if source == 'alphaxiv':
                # alphaxivëŠ” playwright ì‚¬ìš©
                try:
                    from playwright.sync_api import sync_playwright

                    with sync_playwright() as p:
                        browser = p.chromium.launch(headless=True)
                        page = browser.new_page()

                        print(f"   ğŸ­ Playwrightë¡œ í˜ì´ì§€ ë¡œë”© ì¤‘...")
                        page.goto(fetch_url, wait_until="networkidle", timeout=30000)
                        page.wait_for_timeout(2000)  # ì¶”ê°€ ëŒ€ê¸°

                        html = page.content()
                        browser.close()
                except Exception as e:
                    print(f"   âš ï¸  Playwright ë¡œë”© ì‹¤íŒ¨: {e}")
                    # fallback to requests
                    response = session.get(fetch_url, timeout=30)
                    response.raise_for_status()
                    html = response.text
            else:
                # ë‚˜ë¨¸ì§€ëŠ” requests ì‚¬ìš©
                response = session.get(fetch_url, timeout=30)
                response.raise_for_status()
                html = response.text

            # trafilaturaë¡œ markdown ì¶”ì¶œ
            markdown_content = trafilatura.extract(
                html,
                include_comments=False,
                include_tables=True,
                include_links=True,
                include_images=True,  # ì´ë¯¸ì§€ í¬í•¨
                include_formatting=True,
                output_format='markdown',
                url=url
            )

            if not markdown_content:
                print(f"âš ï¸  Content ì¶”ì¶œ ì‹¤íŒ¨: {title}")
                continue

            # hf_blog í—¤ë” í¬ë§· ìˆ˜ì • (ê¹¨ì§„ ë§í¬ ì œê±°)
            # íŒ¨í„´: ### \n [ \n ](url) \n Title â†’ ### Title
            markdown_content = re.sub(
                r'(#{1,6})[\s\n]+\[[\s\n]*\]\([^)]+\)[\s\n]+(.+?)(?=\n)',
                r'\1 \2',
                markdown_content,
                flags=re.MULTILINE
            )

            # ë³¼ë“œ ë’¤ì— ê³µë°± ì—†ì´ ì†Œë¬¸ìê°€ ì˜¤ëŠ” ê²½ìš° ê³µë°± ì¶”ê°€
            # **AssetOpsBench**is â†’ **AssetOpsBench** is
            markdown_content = re.sub(
                r'\*\*([^*]+)\*\*([a-z])',
                r'**\1** \2',
                markdown_content
            )

            # ì—°ì†ëœ ë³¼ë“œ í•­ëª©ë“¤ì„ bullet listë¡œ ë³€í™˜
            def fix_consecutive_bold_items(match):
                line = match.group(0)
                # **content** ë˜ëŠ” **content**text í˜•íƒœë¥¼ ëª¨ë‘ ì°¾ê¸°
                items = re.findall(r'\*\*([^*]+)\*\*([^*]*?)(?=\*\*|$)', line)
                if len(items) >= 2:  # 2ê°œ ì´ìƒì˜ í•­ëª©ì´ ìˆì„ ë•Œë§Œ
                    result = []
                    for bold, text in items:
                        text = text.strip()
                        if text:
                            result.append(f'- **{bold.strip()}** {text}')
                        else:
                            result.append(f'- **{bold.strip()}**')
                    return '\n'.join(result)
                return line

            # íŒ¨í„´1: **content**text**content2**text2 í˜•íƒœ
            # íŒ¨í„´2: **content****content2****content3** í˜•íƒœ (í…ìŠ¤íŠ¸ ì—†ì´ ì—°ì†)
            markdown_content = re.sub(
                r'^(\*\*[^*\n]+\*\*[^*\n]*){2,}$',  # 2ê°œ ì´ìƒì˜ **content** ë˜ëŠ” **content**text
                fix_consecutive_bold_items,
                markdown_content,
                flags=re.MULTILINE
            )

            # YAML frontmatter ìƒì„±
            # ì œëª©ì—ì„œ ë”°ì˜´í‘œ escape
            escaped_title = title.replace('"', '\\"')
            frontmatter = f"""---
title: "{escaped_title}"
url: "{url}"
date: "{date}"
source: "{source}"
---

"""

            # ìµœì¢… markdown íŒŒì¼ ë‚´ìš©
            final_content = frontmatter + markdown_content

            # íŒŒì¼ ì €ì¥
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(final_content)

            print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_file}")
            print(f"   íŒŒì¼ í¬ê¸°: {len(final_content)} bytes")

            # ì†ë„ ì œí•œ (1ì´ˆ ëŒ€ê¸°)
            time.sleep(1)

        except Exception as e:
            print(f"âŒ Error processing {article.get('title', 'unknown')}: {e}")
            import traceback
            traceback.print_exc()


class AITimesCrawler(BaseSourceCrawler):
    """AI Times í¬ë¡¤ëŸ¬ (í•œêµ­)"""

    def __init__(self):
        super().__init__("ai_times", "https://www.aitimes.com/", rate_limit=1.5)

    def parse_listing_page(self, html: str) -> List[Article]:
        """AI Times ëª©ë¡ í˜ì´ì§€ íŒŒì‹±"""
        from lxml import html as lxml_html

        articles = []
        seen_urls = set()
        tree = lxml_html.fromstring(html)

        # article íƒœê·¸ ë‚´ì˜ ë§í¬ë§Œ ì¶”ì¶œ (ë©”ì¸ ë‰´ìŠ¤ ì˜ì—­)
        # ë˜ëŠ” auto-article í´ë˜ìŠ¤ ë‚´ì˜ ë§í¬
        article_links = tree.xpath(
            '//article//a[contains(@href, "articleView.html")] | '
            '//div[contains(@class, "auto-article")]//a[contains(@href, "articleView.html")]'
        )
        print(f"Found {len(article_links)} article links in main sections")

        for link in article_links:
            article = self._parse_news_link(link)
            if article and article.url not in seen_urls:
                # ì¤‘ë³µ ì œê±° ë° íŠ¹ì • ì¹´í…Œê³ ë¦¬ ì œì™¸
                if (len(article.title) > 10 and
                    not article.title.startswith('[AIì›¹íˆ°]') and
                    not article.title.startswith('NOT MY BLOOD')):
                    articles.append(article)
                    seen_urls.add(article.url)

                    # ìµœëŒ€ 30ê°œë§Œ ì¶”ì¶œ
                    if len(articles) >= 30:
                        break

        return articles

    def _parse_news_link(self, link) -> Optional[Article]:
        """ê°œë³„ ë‰´ìŠ¤ ë§í¬ íŒŒì‹±"""
        try:
            title = link.text_content().strip()
            url = link.get('href', '')

            # ì œëª©ì´ ì—†ê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
            if not title or len(title) < 5:
                return None

            if url and not url.startswith('http'):
                url = urljoin(self.base_url, url)

            # ë¶€ëª¨ ìš”ì†Œì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
            parent = link.getparent()
            date = None
            author = None
            description = None

            # ì—¬ëŸ¬ ë‹¨ê³„ ìœ„ë¡œ ì˜¬ë¼ê°€ë©´ì„œ ë©”íƒ€ë°ì´í„° ì°¾ê¸°
            for _ in range(3):
                if parent is None:
                    break

                # ë‚ ì§œ ì°¾ê¸°
                if date is None:
                    date_elem = parent.xpath(".//*[contains(@class, 'date') or contains(@class, 'time') or contains(@class, 'byline')]")
                    if date_elem:
                        date = date_elem[0].text_content().strip()

                # ì €ì ì°¾ê¸°
                if author is None:
                    author_elem = parent.xpath(".//*[contains(@class, 'author') or contains(@class, 'writer')]")
                    if author_elem:
                        author = author_elem[0].text_content().strip()

                # ì„¤ëª… ì°¾ê¸°
                if description is None:
                    desc_elem = parent.xpath(".//dd | .//p")
                    if desc_elem:
                        desc_text = desc_elem[0].text_content().strip()
                        # ì œëª©ê³¼ ë‹¤ë¥¸ ê²½ìš°ë§Œ ì„¤ëª…ìœ¼ë¡œ ì‚¬ìš©
                        if desc_text and desc_text != title:
                            description = desc_text

                parent = parent.getparent()

            return Article(
                title=title,
                url=url,
                date=date,
                author=author,
                description=description
            )
        except Exception as e:
            print(f"Error parsing news link: {e}")
            return None


def test_fetch_top_articles() -> None:
    """ì†ŒìŠ¤ ë³„ ìƒìœ„ 1ê°œ ê¸°ì‚¬ì˜ contentë¥¼ ì¶”ì¶œí•˜ì—¬ ì €ì¥ (í…ŒìŠ¤íŠ¸ìš©)"""
    print(f"\n{'='*60}")
    print("í…ŒìŠ¤íŠ¸: ì†ŒìŠ¤ë³„ ìƒìœ„ 1ê°œ ê¸°ì‚¬ content ì¶”ì¶œ")
    print(f"{'='*60}\n")

    # data/crawled_data/ ì—ì„œ ê°€ì¥ ìµœê·¼ íŒŒì¼ ì°¾ê¸°
    crawled_data_dir = Path("data/crawled_data")
    if not crawled_data_dir.exists():
        print("âŒ data/crawled_data/ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    json_files = list(crawled_data_dir.glob("crawler_results_*.json"))
    if not json_files:
        print("âŒ í¬ë¡¤ë§ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì„ íƒ (íŒŒì¼ëª… ì •ë ¬)
    latest_file = sorted(json_files)[-1]
    print(f"ğŸ“‚ ë¡œë”©: {latest_file}\n")

    # JSON ë¡œë“œ
    with open(latest_file, 'r', encoding='utf-8') as f:
        results = json.load(f)

    # ê° ì†ŒìŠ¤ì—ì„œ ì²« ë²ˆì§¸ ê¸°ì‚¬ ì„ íƒ
    test_articles = []
    for source_data in results:
        source_name = source_data.get('source', 'unknown')
        articles = source_data.get('articles', [])

        if articles:
            # ì²« ë²ˆì§¸ ê¸°ì‚¬ ì„ íƒ
            first_article = articles[0].copy()
            first_article['source'] = source_name  # source ì •ë³´ ì¶”ê°€
            test_articles.append(first_article)
            print(f"âœ“ {source_name}: {first_article.get('title', 'N/A')[:60]}...")

    print(f"\nì´ {len(test_articles)}ê°œ ê¸°ì‚¬ ì„ íƒë¨\n")

    # content ì¶”ì¶œ ë° ì €ì¥
    fetch_article_content(test_articles)

    print(f"\n{'='*60}")
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"{'='*60}")


def extract_introduction(markdown: str, source: str) -> Optional[str]:
    """
    ë§ˆí¬ë‹¤ìš´ì—ì„œ introduction ì¶”ì¶œ
    1. ì²« ë²ˆì§¸ í—¤ë”ë¥¼ ì°¾ëŠ”ë‹¤
    2. í—¤ë” ì´ì „ì— ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ â†’ ê·¸ê²ƒì„ ì‚¬ìš©
    3. í—¤ë”ê°€ ì²« ì¤„ì´ë©´ â†’ ì²« í—¤ë” ë‹¤ìŒë¶€í„° ë‘ ë²ˆì§¸ í—¤ë”ê¹Œì§€ ë˜ëŠ” ì²« ì´ì¤‘ ê°œí–‰ê¹Œì§€
    4. í—¤ë”ê°€ ì—†ìœ¼ë©´ â†’ ì²« ì´ì¤‘ ê°œí–‰ ì´ì „ê¹Œì§€
    5. ëª¨ë‘ ì—†ìœ¼ë©´ â†’ ìµœëŒ€ 500ì
    """
    if not markdown:
        return None

    try:
        # ì²« ë²ˆì§¸ í—¤ë” ì°¾ê¸°
        first_header = re.search(r'^#{1,6}\s', markdown, re.MULTILINE)

        if first_header:
            # í—¤ë” ì´ì „ í…ìŠ¤íŠ¸ í™•ì¸
            before_header = markdown[:first_header.start()].strip()

            if before_header and len(before_header) > 20:
                # í—¤ë” ì´ì „ì— ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
                intro = before_header
            else:
                # í—¤ë”ê°€ ì²« ì¤„ì´ë©´, í—¤ë” ë‹¤ìŒë¶€í„° ì¶”ì¶œ
                after_header_start = first_header.end()
                rest_content = markdown[after_header_start:].strip()

                # ë‘ ë²ˆì§¸ í—¤ë” ì°¾ê¸°
                second_header = re.search(r'^#{1,6}\s', rest_content, re.MULTILINE)

                if second_header:
                    # ë‘ ë²ˆì§¸ í—¤ë” ì´ì „ê¹Œì§€
                    intro = rest_content[:second_header.start()].strip()
                elif '\n\n' in rest_content:
                    # ì²« ì´ì¤‘ ê°œí–‰ ì´ì „ê¹Œì§€
                    intro = rest_content.split('\n\n')[0].strip()
                else:
                    # ì²« 500ì
                    intro = rest_content[:500].strip()
        else:
            # í—¤ë”ê°€ ì—†ìœ¼ë©´
            if '\n\n' in markdown:
                intro = markdown.split('\n\n')[0].strip()
            else:
                intro = markdown[:500].strip()

        return intro if intro else None
    except Exception as e:
        print(f"  âš ï¸  Introduction ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None


async def fetch_description_async(session: aiohttp.ClientSession, url: str, source: str) -> Optional[str]:
    """ë¹„ë™ê¸°ë¡œ URLì—ì„œ description ì¶”ì¶œ"""
    try:
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as response:
            if response.status != 200:
                return None

            html = await response.text()

            # trafilaturaë¡œ markdown ì¶”ì¶œ (ë™ê¸° í•¨ìˆ˜ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
            markdown_content = trafilatura.extract(
                html,
                include_comments=False,
                include_tables=False,
                include_links=False,
                include_images=False,
                include_formatting=False,
                output_format='markdown',
                url=url
            )

            if not markdown_content:
                return None

            # Introduction ì¶”ì¶œ
            intro = extract_introduction(markdown_content, source)
            return intro

    except asyncio.TimeoutError:
        print(f"  â±ï¸  Timeout: {url}")
        return None
    except Exception as e:
        print(f"  âš ï¸  Error fetching {url}: {e}")
        return None


async def enrich_articles_with_descriptions(articles: List[Dict[str, Any]], source: str) -> List[Dict[str, Any]]:
    """ê¸°ì‚¬ ëª©ë¡ì— description ì¶”ê°€ (ë³‘ë ¬ ì²˜ë¦¬)"""
    if not articles:
        return articles

    print(f"\nğŸ“ Fetching descriptions for {source} ({len(articles)} articles)...")

    # aiohttp session ìƒì„±
    connector = aiohttp.TCPConnector(limit=10)  # ë™ì‹œ ì—°ê²° ì œí•œ
    async with aiohttp.ClientSession(
        connector=connector,
        headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    ) as session:
        # ëª¨ë“  ê¸°ì‚¬ì— ëŒ€í•´ ë³‘ë ¬ë¡œ description ì¶”ì¶œ
        tasks = []
        for article in articles:
            url = article.get('url')
            if url and not article.get('description'):  # descriptionì´ ì—†ëŠ” ê²½ìš°ë§Œ
                task = fetch_description_async(session, url, source)
                tasks.append((article, task))
            else:
                tasks.append((article, None))

        # ë³‘ë ¬ ì‹¤í–‰
        for i, (article, task) in enumerate(tasks):
            if task:
                description = await task
                if description:
                    article['description'] = description
                    print(f"  âœ“ [{i+1}/{len(articles)}] {article.get('title', 'N/A')[:50]}...")
                else:
                    print(f"  âœ— [{i+1}/{len(articles)}] {article.get('title', 'N/A')[:50]}...")

    print(f"âœ… Description fetching complete for {source}")
    return articles


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (YYMMDD_hhmm í˜•ì‹)
    timestamp = datetime.now().strftime("%y%m%d_%H%M")

    # ì €ì¥ ê²½ë¡œ ì„¤ì •
    output_dir = Path("data/crawled_data")
    output_dir.mkdir(parents=True, exist_ok=True)

    output_file = output_dir / f"crawler_results_{timestamp}.json"

    # ë™ì¼í•œ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if output_file.exists():
        print(f"\n{'='*60}")
        print(f"í¬ë¡¤ë§ ê²°ê³¼ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {output_file}")
        print(f"ë™ì¼í•œ ì‹œê°„ëŒ€ì— ì´ë¯¸ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ë¶ˆí•„ìš”í•œ í¬ë¡¤ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        print(f"{'='*60}\n")
        return

    crawlers = [
        AlphaXivCrawler(),
        HuggingFaceBlogCrawler(),
        VentureBeatCrawler(),
        AITimesCrawler(),
    ]

    results = []

    for crawler in crawlers:
        try:
            result = crawler.crawl(full_content=False)

            # Description enrichment (ë¹„ë™ê¸°)
            # alphaxivëŠ” ì´ë¯¸ descriptionì´ ìˆìœ¼ë¯€ë¡œ ì œì™¸
            source_name = result['source']
            articles = result.get('articles', [])

            if articles and source_name != 'alphaxiv':
                # asyncioë¡œ ë³‘ë ¬ ì²˜ë¦¬
                enriched_articles = asyncio.run(
                    enrich_articles_with_descriptions(articles, source_name)
                )
                result['articles'] = enriched_articles

            results.append(result)

            # ê²°ê³¼ ì¶œë ¥
            print(f"\n{'='*60}")
            print(f"Results for {result['source']}:")
            print(f"{'='*60}")
            print(f"Total articles: {len(result.get('articles', []))}")
            articles_with_desc = sum(1 for a in result.get('articles', []) if a.get('description'))
            print(f"Articles with description: {articles_with_desc}")
            print()

        except Exception as e:
            print(f"Error crawling {crawler.source_name}: {e}")
            import traceback
            traceback.print_exc()

    # ì „ì²´ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\n{'='*60}")
    print(f"All results saved to: {output_file}")
    print(f"Total sources crawled: {len(results)}")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()
