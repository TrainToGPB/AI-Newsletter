#!/usr/bin/env python3
"""
AI Newsletter Curator System
LangChain + ChatOpenAIë¥¼ ì‚¬ìš©í•˜ì—¬ í¬ë¡¤ë§ëœ ê¸°ì‚¬ë¥¼ íë ˆì´ì…˜í•©ë‹ˆë‹¤.
"""

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

from models import CurationResult, SelectedArticle
from crawler import fetch_article_content


def load_prompt(path: str) -> str:
    """í”„ë¡¬í”„íŠ¸ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë¡œë“œ"""
    with open(path, 'r', encoding='utf-8') as f:
        return f.read()


def load_latest_crawled_data() -> Dict[str, Any]:
    """
    data/crawled_data/ì—ì„œ ê°€ì¥ ìµœê·¼ í¬ë¡¤ë§ ê²°ê³¼ ë¡œë“œ

    Returns:
        {"alphaxiv": [...], "hf_blog": [...], "venturebeat": [...], "ai_times": [...]}
    """
    crawled_data_dir = Path("data/crawled_data")
    if not crawled_data_dir.exists():
        raise FileNotFoundError(
            f"Crawled data directory not found: {crawled_data_dir}\n"
            "Please run crawler.py first."
        )

    # ê°€ì¥ ìµœê·¼ JSON íŒŒì¼ ì°¾ê¸°
    json_files = list(crawled_data_dir.glob("crawler_results_*.json"))
    if not json_files:
        raise FileNotFoundError(
            f"No crawler results found in {crawled_data_dir}\n"
            "Please run crawler.py first."
        )

    latest_file = sorted(json_files)[-1]
    print(f"ğŸ“‚ Loading crawled data: {latest_file}")

    with open(latest_file, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)

    # ì†ŒìŠ¤ë³„ë¡œ ì¬êµ¬ì„±
    data_by_source = {}
    for source_data in raw_data:
        source_name = source_data.get('source', 'unknown')
        articles = source_data.get('articles', [])
        data_by_source[source_name] = articles
        print(f"  âœ“ {source_name}: {len(articles)} articles")

    return data_by_source


def apply_duplicate_filtering(data_by_source: Dict[str, List[Dict]]) -> Dict[str, List[Dict]]:
    """
    Filter out articles that were already sent in past newsletters

    Args:
        data_by_source: Dict mapping source name to article list

    Returns:
        Filtered dict with duplicates removed
    """
    from memory import load_sent_urls, filter_duplicate_articles, get_duplicate_stats

    print(f"\nğŸ” Checking for duplicate articles...")
    try:
        sent_urls = load_sent_urls(days_back=14)
        print(f"   Loaded {len(sent_urls)} URLs from past 2 weeks")

        # Log statistics
        stats = get_duplicate_stats(data_by_source, sent_urls)
        for source, counts in stats.items():
            if counts['duplicates'] > 0:
                print(f"   âš ï¸  {source}: {counts['duplicates']}/{counts['total']} duplicates")
            else:
                print(f"   âœ“ {source}: {counts['new']}/{counts['total']} new articles")

        # Filter duplicates
        data_by_source, dup_counts = filter_duplicate_articles(data_by_source, sent_urls)

        total_filtered = sum(dup_counts.values())
        if total_filtered > 0:
            print(f"   ğŸš« Filtered {total_filtered} duplicate articles")
        else:
            print(f"   âœ… No duplicates found")

    except FileNotFoundError:
        print(f"   â„¹ï¸  No previous newsletters found (first run)")
    except Exception as e:
        print(f"   âš ï¸  Error loading memory: {e}")
        print(f"   â„¹ï¸  Continuing without duplicate filtering")

    return data_by_source


def format_articles_xml(articles: List[Dict[str, Any]]) -> str:
    """
    ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ë¥¼ XML í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…

    Args:
        articles: ê¸°ì‚¬ ëª©ë¡
        source: ì†ŒìŠ¤ ì´ë¦„

    Returns:
        XML í˜•ì‹ ë¬¸ìì—´
    """
    xml_parts = []
    for idx, article in enumerate(articles):
        title = article.get('title', 'No title')
        url = article.get('url', '')
        date = article.get('date', '')
        description = article.get('description', '')

        xml_parts.append(f"<article index='{idx}'>")
        xml_parts.append(f"<title>{title}</title>")
        xml_parts.append(f"<url>{url}</url>")
        xml_parts.append(f"<date>{date}</date>")
        if description:
            xml_parts.append(f"<description>{description}</description>")
        xml_parts.append(f"</article>")
    
    return '\n'.join(xml_parts)


def enrich_selected_articles(
    result: CurationResult,
    articles_by_source: Dict[str, List[Dict[str, Any]]]
) -> List[Dict[str, Any]]:
    """
    LLM ê²°ê³¼ì— ì›ë³¸ ë°ì´í„°ì˜ url, description ì¶”ê°€

    Args:
        result: CurationResult (LLM ì‘ë‹µ)
        articles_by_source: ì†ŒìŠ¤ë³„ ê¸°ì‚¬ ëª©ë¡

    Returns:
        enriched articles with url and description
    """
    enriched = []
    for article in result.selected_articles:
        source_articles = articles_by_source.get(article.source, [])
        if article.index < len(source_articles):
            original = source_articles[article.index]
            enriched.append({
                "source": article.source,
                "title": article.title,
                "url": original.get("url", ""),
                "description": original.get("description", ""),
                "reason_for_selection": article.reason_for_selection
            })
    return enriched


def curate_category(
    llm: ChatOpenAI,
    prompt_template: str,
    articles_by_source: Dict[str, List[Dict[str, Any]]],
    category: str
) -> CurationResult:
    """
    ì¹´í…Œê³ ë¦¬ë³„ íë ˆì´ì…˜ ìˆ˜í–‰

    Args:
        llm: ChatOpenAI ëª¨ë¸
        prompt_template: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ('{articles_xml}' í¬í•¨)
        articles_by_source: ì†ŒìŠ¤ë³„ ê¸°ì‚¬ ëª©ë¡
        category: "academic" ë˜ëŠ” "technews"

    Returns:
        CurationResult
    """
    # ëª¨ë“  ì†ŒìŠ¤ì˜ ê¸°ì‚¬ë¥¼ XMLë¡œ ë³€í™˜
    xml_parts = []
    for source, articles in articles_by_source.items():
        if articles:
            xml_parts.append(format_articles_xml(articles))

    articles_xml = '\n\n'.join(xml_parts)

    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = prompt_template.replace('{articles_xml}', articles_xml)

    # LLM with structured output
    structured_llm = llm.with_structured_output(CurationResult)

    print(f"\nğŸ¤– Curating {category}...")
    print(f"   Total sources: {len(articles_by_source)}")
    print(f"   Total articles: {sum(len(articles) for articles in articles_by_source.values())}")

    # íë ˆì´ì…˜ ì‹¤í–‰
    result = structured_llm.invoke(prompt)

    print(f"   âœ“ Selected {len(result.selected_articles)} articles")

    return result


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()

    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError(
            "OPENAI_API_KEY not found in environment.\n"
            "Please create a .env file with: OPENAI_API_KEY=your_key_here"
        )

    print(f"\n{'='*60}")
    print("AI Newsletter Curator")
    print(f"{'='*60}\n")

    # 1. í¬ë¡¤ë§ ë°ì´í„° ë¡œë“œ
    crawled_data = load_latest_crawled_data()

    # 1.5. ì¤‘ë³µ í•„í„°ë§ (ìµœê·¼ 2ì£¼ê°„ ë‰´ìŠ¤ë ˆí„°ì— ë‚˜ê°„ ê¸°ì‚¬ ì œì™¸)
    crawled_data = apply_duplicate_filtering(crawled_data)

    # 2. LLM ì´ˆê¸°í™” (gpt-5-mini)
    print(f"\nğŸ”§ Initializing LLM (gpt-5-mini)...")
    llm = ChatOpenAI(
        model="gpt-5-mini",
        model_kwargs={
            "reasoning_effort": "medium",  # low, medium, high
            "max_completion_tokens": 2000
        }
    )
    print("   âœ“ LLM initialized")

    # 3. í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    academic_prompt = load_prompt("prompts/academic_curator.md")
    technews_prompt = load_prompt("prompts/technews_curator.md")

    # 4. Academic íë ˆì´ì…˜ (alphaxiv + hf_blog)
    academic_sources = {
        "alphaxiv": crawled_data.get("alphaxiv", []),
        "hf_blog": crawled_data.get("hf_blog", [])
    }

    academic_result = curate_category(
        llm=llm,
        prompt_template=academic_prompt,
        articles_by_source=academic_sources,
        category="academic"
    )

    # 5. Tech News íë ˆì´ì…˜ (venturebeat + ai_times)
    technews_sources = {
        "venturebeat": crawled_data.get("venturebeat", []),
        "ai_times": crawled_data.get("ai_times", [])
    }

    technews_result = curate_category(
        llm=llm,
        prompt_template=technews_prompt,
        articles_by_source=technews_sources,
        category="technews"
    )

    # 6. ê²°ê³¼ ì €ì¥ (url, description í¬í•¨)
    timestamp = datetime.now().strftime("%y%m%d_%H%M")
    output_dir = Path("data/curated")
    output_dir.mkdir(parents=True, exist_ok=True)

    output_file = output_dir / f"curated_{timestamp}.json"

    # Enrich with url and description
    academic_enriched = enrich_selected_articles(academic_result, academic_sources)
    technews_enriched = enrich_selected_articles(technews_result, technews_sources)

    # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
    output_data = {
        "timestamp": datetime.now().isoformat(),
        "academic": {
            "category": "academic",
            "selected_articles": academic_enriched
        },
        "technews": {
            "category": "technews",
            "selected_articles": technews_enriched
        }
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print(f"\n{'='*60}")
    print(f"âœ… Curation Complete!")
    print(f"{'='*60}")
    print(f"\nğŸ“„ Results saved to: {output_file}")
    print(f"\nğŸ“Š Summary:")
    print(f"   Academic: {len(academic_enriched)} articles selected")
    for article in academic_enriched:
        print(f"      â€¢ [{article['source']}] {article['title'][:60]}...")
    print(f"\n   Tech News: {len(technews_enriched)} articles selected")
    for article in technews_enriched:
        print(f"      â€¢ [{article['source']}] {article['title'][:60]}...")
    print(f"\n{'='*60}\n")

    # 7. íë ˆì´ì…˜ëœ ê¸°ì‚¬ë“¤ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì €ì¥
    print(f"\n{'='*60}")
    print("ğŸ“ Saving curated articles as markdown...")
    print(f"{'='*60}\n")

    all_selected_articles = academic_enriched + technews_enriched
    fetch_article_content(all_selected_articles)


if __name__ == "__main__":
    main()
